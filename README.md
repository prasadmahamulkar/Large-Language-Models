Updated on 14th August 2024
# Large Language Models Notebooks
<p>
 <a href="https://medium.com/@prasadmahamulkar">Article</a> â€¢ Models & Datasets on: ðŸ¤—<a href="https://huggingface.co/prsdm">Hugging Face</a>
</p>
<p align="centre"> This repository contains all the large language models that I have worked on </p>

 ![llm tree-min](https://github.com/prasadmahamulkar/Large-Language-Models-llm-/assets/93597510/9da2115a-3eed-4f5f-ac72-125800a0eb6e)
 

## Fine-tune Projects
<a href="https://medium.com/@prasadmahamulkar/fine-tuning-phi-2-a-step-by-step-guide-e672e7f1d009">Article</a>
<p> Fine-tuning or instruction tuning is the process where the pre-trained model is further trained on the smaller dataset to adapt its knowledge for a specific task or domain. This process tweaks the modelâ€™s parameters to perform 

  | LLMs                      |Description| Dataset | Notebooks | 
|----------------------------|------------------------|-----------------------|-----------------------|
|  [Phi-2](https://huggingface.co/prsdm/phi-2-medquad)  |    This model has been fine-tuned on a Medical dataset to answer questions related to diseases and symptoms. (used SFT method)             | [Dataset](https://huggingface.co/datasets/prsdm/MedQuad-phi2-1k)                 | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/prasadmahamulkar/Large-Language-Models/blob/main/Fine_tune_Phi_2_on_Google_Colab_.ipynb)               |       
|  [llama-2](https://huggingface.co/prsdm/llama-2-finance)  |    This model has been fine-tuned on a dataset with human-generated prompts to answer questions related to general knowledge.  (used SFT method)    | [Dataset](https://huggingface.co/datasets/prsdm/finance-llama2-1k)                 | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/prasadmahamulkar/Large-Language-Models/blob/main/Fine_tune_llama_2_on_Colab.ipynb)               |   
